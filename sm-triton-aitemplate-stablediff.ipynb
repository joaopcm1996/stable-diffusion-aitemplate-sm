{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dab554c6",
   "metadata": {},
   "source": [
    "# Deploy Stable Diffusion on a SageMaker GPU Multi-Model Endpoint with Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5640df9b",
   "metadata": {},
   "source": [
    "In this notebook we will host Stable Diffusion  SageMaker GPU Multi-Model Endpoints (MME GPU) powered by NVIDIA Triton Inference Server. We will compile Stable Diffusion for lower latency using [AITemplate](https://github.com/facebookincubator/AITemplate).\n",
    "\n",
    "Skip to:\n",
    "1. [Installs and imports](#installs)\n",
    "2. [Packaging a conda environment, extending Sagemaker Triton container](#condaenv)\n",
    "3. [Compile model with AITemplate](#aitemplate)\n",
    "4. [Local testing of Triton model repository](#local)\n",
    "5. [Deploy to SageMaker Real-Time Endpoint](#deploy)\n",
    "6. [Analyze endpoint logs](#logs)\n",
    "7. [Clean up](#cleanup)\n",
    "------\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf35ff",
   "metadata": {},
   "source": [
    "### Part 1 - Installs and imports <a name=\"installs\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df6cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]\n",
    "!pip install -U sagemaker ipywidgets pillow numpy transformers accelerate diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c48876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import *\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# variables\n",
    "s3_client = boto3.client(\"s3\")\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "# sagemaker variables\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session())\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d58e0e",
   "metadata": {},
   "source": [
    "### Part 2 - Packaging a conda environment, extending Sagemaker Triton container <a name=\"condaenv\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8973a7d2",
   "metadata": {},
   "source": [
    "When using the Triton Python backend (which our Stable Diffusion model will run on), you can include your own environment and dependencies. The recommended way to do this is to use [conda pack](https://conda.github.io/conda-pack/) to generate a conda environment archive in `tar.gz` format, and point to it in the `config.pbtxt` file of the models that should use it, adding the snippet: \n",
    "\n",
    "```\n",
    "parameters: {\n",
    "  key: \"EXECUTION_ENV_PATH\",\n",
    "  value: {string_value: \"path_to_your_env.tar.gz\"}\n",
    "}\n",
    "\n",
    "```\n",
    "You can use a different environment with every new loaded model, or the same for all models loaded into the container (read more on this [here](https://github.com/triton-inference-server/python_backend#creating-custom-execution-environments)). We will extend the public SageMaker Triton container image to include our environment, to avoid increasing the model S3 download time. \n",
    "\n",
    "Let's start by creating the conda environment with the necessary dependencies; this script will output a `stablediff_env.tar.gz` file.\n",
    "\n",
    "We pass the conda path because we change a file in an installed library (AITemplate) to suppport A10G GPU's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82382b39-fd91-4f71-8248-79bd3067d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture conda_path\n",
    "!echo $CONDA_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f3586-7f4a-4480-9f1f-eb7b8438f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = str(conda_path).strip().split('/')[1:]\n",
    "conda_path= '/'+'/'.join(temp_path[:-1])\n",
    "print(conda_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f523d03",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd docker && bash conda_dependencies.sh \"$conda_path\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551fa2a6-3562-49aa-88bc-e35c2cf75c79",
   "metadata": {},
   "source": [
    "Now, we get the correct URI for the SageMaker Triton container image. Check out all the available Deep Learning Container images that AWS maintains [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b707c2-f78e-452a-9e99-4860232bd76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account mapping for SageMaker Triton Image\n",
    "account_id_map = {\n",
    "    \"us-east-1\": \"785573368785\",\n",
    "    \"us-east-2\": \"007439368137\",\n",
    "    \"us-west-1\": \"710691900526\",\n",
    "    \"us-west-2\": \"301217895009\",\n",
    "    \"eu-west-1\": \"802834080501\",\n",
    "    \"eu-west-2\": \"205493899709\",\n",
    "    \"eu-west-3\": \"254080097072\",\n",
    "    \"eu-north-1\": \"601324751636\",\n",
    "    \"eu-south-1\": \"966458181534\",\n",
    "    \"eu-central-1\": \"746233611703\",\n",
    "    \"ap-east-1\": \"110948597952\",\n",
    "    \"ap-south-1\": \"763008648453\",\n",
    "    \"ap-northeast-1\": \"941853720454\",\n",
    "    \"ap-northeast-2\": \"151534178276\",\n",
    "    \"ap-southeast-1\": \"324986816169\",\n",
    "    \"ap-southeast-2\": \"355873309152\",\n",
    "    \"cn-northwest-1\": \"474822919863\",\n",
    "    \"cn-north-1\": \"472730292857\",\n",
    "    \"sa-east-1\": \"756306329178\",\n",
    "    \"ca-central-1\": \"464438896020\",\n",
    "    \"me-south-1\": \"836785723513\",\n",
    "    \"af-south-1\": \"774647643957\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = (\n",
    "    \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.12-py3\".format(\n",
    "        account_id=account_id_map[region], region=region, base=base\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d498436a-9ee9-46aa-b7f7-e23ea073f710",
   "metadata": {},
   "source": [
    "We then build our extended image, which does nothing more than to copy the packaged environment into the container. Let's check out the Dockerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f77e62f-f7bb-4e65-9f8e-07f037a3ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat docker/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ec85f-a0ff-4174-9cca-c06105d3055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this var to change the name of new container image\n",
    "new_image_name = 'sagemaker-tritonserver-stablediffusion'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75dff8c-54fd-4f2c-b18e-ee5440aa0978",
   "metadata": {},
   "source": [
    "We catch the docker build process' output so that we can easily capture the output container image URI, and check for build errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f5d52-3a04-48b3-82d7-8138293f5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture build_output\n",
    "!cd docker && bash build_and_push.sh \"$new_image_name\" 22.12 \"$mme_triton_image_uri\" \"$region\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfea1b4-7056-40a9-959c-ada1a7b325d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Error response from daemon' in str(build_output):\n",
    "    print(build_output)\n",
    "    raise SystemExit('\\n\\n!!There was an error with the container build!!')\n",
    "else:\n",
    "    extended_triton_image_uri = str(build_output).strip().split('\\n')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a28fb2c-09a5-40a1-bf29-076d8e6fe18b",
   "metadata": {},
   "source": [
    "If the previous cell failed, check the docker build logs to understand the error problem, and read the possible resolution in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289313a4-ac2b-45b9-89ec-efa0f1de81e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If the cell above fails (check out the build_output) because of missing permissions to pull the public Triton base container image,\n",
    "uncomment the commands in this cell, run them and retry the build\n",
    "\"\"\"\n",
    "# mapped_region_account = account_id_map[region]\n",
    "# !aws ecr get-login-password --region \"$region\" | docker login --username AWS --password-stdin \"$mapped_region_account\".dkr.ecr.\"$region\".amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c5d92f",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3e6ff9-4bc3-40d5-b131-7b64160b2cd8",
   "metadata": {},
   "source": [
    "### Part 3 - Compile model with AITemplate <a name=\"aitemplate\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660b803c-c2f2-4d35-8d6d-10c9cbc4a8a2",
   "metadata": {},
   "source": [
    "The next cell will use AITemplate to compile the StableDiffusion 2.1 base model and move it to the Triton model repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b971890",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"model_repo_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfbdc06-b77b-4b7b-a708-3c851462a6a8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker run --gpus=all -it --shm-size=4G --rm -p8000:8000 -p8001:8001 -p8002:8002 -v$(pwd):/model_repository $extended_triton_image_uri /bin/bash /model_repository/workspace/compile_model.sh \"$repo_name\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b63de28",
   "metadata": {},
   "source": [
    "------\n",
    "------\n",
    "### Part 4 - Local testing of Triton model repository <a name=\"local\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3e6815",
   "metadata": {},
   "source": [
    "Now you can test the model repository and validate it is working. Let's run the Triton docker container locally and invoke the model to check this. We are running the Triton container in detached model with the `-d` flag so that it runs in the background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c92b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --gpus=all -d --shm-size=4G --rm -p8000:8000 -p8001:8001 -p8002:8002 -v$(pwd)/$repo_name:/model_repository $extended_triton_image_uri tritonserver --model-repository=/model_repository --exit-on-error=false\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b90d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTAINER_ID=!docker container ls -q\n",
    "FIRST_CONTAINER_ID = CONTAINER_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bbf319",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $FIRST_CONTAINER_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker logs $FIRST_CONTAINER_ID "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580605b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Warning</b>: Rerun the cell above to check the container logs until you verify that Triton has loaded all models successfully, otherwise inference request will fail.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd938e2",
   "metadata": {},
   "source": [
    "#### Now we will invoke the script locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e1371",
   "metadata": {},
   "source": [
    "We will use Triton's HTTP client and its utility functions to send a request to `localhost:8000`, where the server is listening. We are sending text as binary data for input and receiving an array that we decode with numpy as output. Check out the code in `model_repository/pipeline/1/model.py` to understand how the input data is decoded and the output data returned, and check out more Triton Python backend [docs](https://github.com/triton-inference-server/python_backend) and [examples](https://github.com/triton-inference-server/python_backend/tree/main/examples) to understand how to handle other data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8655dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = httpclient.InferenceServerClient(url=\"localhost:8000\")\n",
    "\n",
    "prompt = \"Pikachu in a detective trench coat, photorealistic, nikon\"\n",
    "text_obj = np.array([prompt], dtype=\"object\").reshape((-1, 1))\n",
    "\n",
    "input_text = httpclient.InferInput(\"prompt\", text_obj.shape, np_to_triton_dtype(text_obj.dtype))\n",
    "\n",
    "input_text.set_data_from_numpy(text_obj)\n",
    "\n",
    "output_img = httpclient.InferRequestedOutput(\"generated_image\")\n",
    "\n",
    "start = time.time()\n",
    "query_response = client.infer(model_name=\"pipeline_0\", inputs=[input_text], outputs=[output_img])\n",
    "print(f\"took {time.time()-start} seconds\")\n",
    "\n",
    "image = query_response.as_numpy(\"generated_image\")\n",
    "im = Image.fromarray(np.squeeze(image))\n",
    "im.save(\"generated_image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c7d9e7-53a2-44e4-bb64-10b697aeb04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b05c7a-d37c-441d-9a00-113df7ad8b5e",
   "metadata": {},
   "source": [
    "Let's stop the container that is running locally so we don't take up notebook resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ba306-35e8-43f6-aa18-324de79ed87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker kill $FIRST_CONTAINER_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f450772c-0561-4df1-857b-787c9dfdf889",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "### Part 5 - Deploy to SageMaker Real-Time Endpoint <a name=\"deploy\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393bf7e3-e648-4625-94d4-e0284a87e58e",
   "metadata": {},
   "source": [
    "SageMaker expects a .tar.gz file containing the Triton model repository to be hosted on the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e5f2d-e658-43b0-be9e-f2d8527e68a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'stable-diffusion-aitemplate'\n",
    "tar_file_name = 'sd2-aitemplate.tar.gz'\n",
    "!tar -C model_repo_0/ -czf \"$tar_file_name\" .\n",
    "model_url = sagemaker_session.upload_data(path=tar_file_name, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c4844-d19c-4e4b-9634-63df5cd41464",
   "metadata": {},
   "source": [
    "Create SM container and model definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e328daa-3d5c-4c66-a6cc-7c80c4274dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": extended_triton_image_uri,\n",
    "    # \"Image\": mme_triton_image_uri,\n",
    "    \"ModelDataUrl\": model_url,\n",
    "    \"Mode\": \"SingleModel\",\n",
    "    \"Environment\": {\n",
    "        \"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"pipeline_0\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbcc545-a421-438c-b01f-e6420a504a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = f\"{prefix}-mdl-{ts}\"\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb4c11a-4f8a-409e-832e-f27b8be07bb7",
   "metadata": {},
   "source": [
    "Create a SageMaker endpoint configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c9c1a6-92de-44d6-8b0b-7c6f388da957",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{prefix}-epc-{ts}\"\n",
    "instance_type = 'ml.g5.xlarge'\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f09a802-706e-49f4-ae36-cb4e32960b6f",
   "metadata": {},
   "source": [
    "Create the endpoint, and wait for it to be up and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39066acd-e31e-433d-9200-65f7bb3b6975",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f\"{prefix}-ep-{ts}\"\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfee5c2-049d-4c96-89ea-725f99003bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff32048-bf34-4b27-abe7-580929bdbe39",
   "metadata": {},
   "source": [
    "#### Invoke models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359093f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Smiling person\"\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "text_obj = np.array([prompt], dtype=\"object\").reshape((-1, 1))\n",
    "\n",
    "inputs.append(httpclient.InferInput(\"prompt\", text_obj.shape, np_to_triton_dtype(text_obj.dtype)))\n",
    "inputs[0].set_data_from_numpy(text_obj)\n",
    "\n",
    "\n",
    "outputs.append(httpclient.InferRequestedOutput(\"generated_image\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5980bb7-5ad5-4eae-9dbd-229d0bd37806",
   "metadata": {},
   "source": [
    "Since we are using the SageMaker Runtime client to send an HTTP request to the endpoint now, we use Triton's `generate_request_body` method to create the right [request format](https://github.com/triton-inference-server/server/tree/main/docs/protocol) for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50b4011-5d1b-4256-a0aa-1d89082a3941",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_body, header_length = httpclient.InferenceServerClient.generate_request_body(\n",
    "    inputs, outputs=outputs\n",
    ")\n",
    "\n",
    "print(request_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcac9f5-1c13-4c10-b0a1-578cc8f661d3",
   "metadata": {},
   "source": [
    "We are sending our request in binary format for lower inference latency. \n",
    "\n",
    "With the binary+json format, we have to specify the length of the request metadata in the header to allow Triton to correctly parse the binary payload. This is done using a custom Content-Type header, which is different from using an `Inference-Header-Content-Length` header on a standalone Triton server because custom headers aren’t allowed in SageMaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee19f36-c0a1-479f-8e73-beeaf180a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "for i in range(20):\n",
    "    tick = time.time()\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/vnd.sagemaker-triton.binary+json;json-header-size={}\".format(header_length),\n",
    "            Body=request_body,\n",
    "        )\n",
    "    print(time.time()-tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf89a726-5ac4-4fab-8705-3d0aa7263e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_length_str = response[\"ContentType\"][len(header_length_prefix) :]\n",
    "result = httpclient.InferenceServerClient.parse_response_body(\n",
    "            response[\"Body\"].read(), header_length=int(header_length_str))\n",
    "image_array = result.as_numpy(\"generated_image\")\n",
    "image = Image.fromarray(np.squeeze(image_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6dbb29-9279-4121-8377-a23ece9a255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46fa56-abd2-4dca-b0a7-9e9db96b0697",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "### Part 6 - Analyze endpoint logs <a name=\"logs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b1264-80fa-468c-b66e-eceb0f80f63a",
   "metadata": {},
   "source": [
    "Let's analyze our endpoint's CloudWatch logs and verify the behaviour triggered by MME: as the GPU ran out of memory space, the first models we invoked are unloaded to make room for the ones invoked later. MME follows a Least Recently Used (LRU) policy to evict models from GPU memory or RAM (in the case of MME on CPU).\n",
    "\n",
    "First we build the URL where we can access our endpoint's logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c31744-33b8-4680-9e56-99c7794f169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudwatch_log_url = f'https://{region}.console.aws.amazon.com/cloudwatch/home?region={region}#logStream:group=/aws/sagemaker/Endpoints/{endpoint_name}'\n",
    "\n",
    "print('↓↓↓Click the following link to access the endpoint logs↓↓↓\\n')\n",
    "print(cloudwatch_log_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839fef3c-3d7f-40f0-b58c-fb5ca344f995",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "### Part 7 - Clean up <a name=\"cleanup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2688904b-426a-4d3b-8e7a-30701f0f752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=sm_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
